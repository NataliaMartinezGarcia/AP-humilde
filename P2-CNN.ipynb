{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec71c9c",
   "metadata": {},
   "source": [
    "# Práctica 2 - CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558c928",
   "metadata": {},
   "source": [
    "### Natalia Martínez García, Lucía Vega Navarrete\n",
    "### Grupo: AP.11.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a60e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import layers, models, regularizers, metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e998d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a2c63",
   "metadata": {},
   "source": [
    "### 1. Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea029d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INFORMACIÓN DEL DATASET\n",
      "==================================================\n",
      "NOMBRE: stl10\n",
      "\n",
      "IMÁGENES:\n",
      " - Dimensiones: (96, 96, 3)\n",
      " - Tipo: <class 'numpy.uint8'>\n",
      " - Longitud aplanada: 27648\n",
      "\n",
      "ETIQUETAS:\n",
      " - Número de clases: 10\n",
      " - Clases: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck\n",
      "\n",
      "SPLITS:\n",
      " - Train: 5,000 imágenes\n",
      " - Test: 8,000 imágenes\n",
      " - Unlabelled: 100,000 imágenes (NO LOS USAMOS)\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset STL-10 ya dividido en entrenamiento y test\n",
    "(train, test), info_ds = tfds.load(\n",
    "    'stl10',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,  # devuelve tuplas (imagen, etiqueta)\n",
    "    with_info=True # devuelve info extra del dataset (número de clases, tamaño de imagen, etc.)\n",
    ")\n",
    "\n",
    "num_clases = info_ds.features['label'].num_classes\n",
    "nombres_clases = info_ds.features['label'].names\n",
    "tamano_imagen = info_ds.features['image'].shape\n",
    "dimension_entrada = np.prod(tamano_imagen)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INFORMACIÓN DEL DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"NOMBRE: {info_ds.name}\")\n",
    "print(f\"\\nIMÁGENES:\")\n",
    "print(f\" - Dimensiones: {tamano_imagen}\")\n",
    "print(f\" - Tipo: {info_ds.features['image'].numpy_dtype}\")\n",
    "print(f\" - Longitud aplanada: {dimension_entrada}\")\n",
    "\n",
    "\n",
    "print(f\"\\nETIQUETAS:\")\n",
    "print(f\" - Número de clases: {num_clases}\")\n",
    "print(f\" - Clases: {', '.join(nombres_clases)}\")\n",
    "\n",
    "print(f\"\\nSPLITS:\")\n",
    "print(f\" - Train: {info_ds.splits['train'].num_examples:,} imágenes\")\n",
    "print(f\" - Test: {info_ds.splits['test'].num_examples:,} imágenes\")\n",
    "print(f\" - Unlabelled: {info_ds.splits['unlabelled'].num_examples:,} imágenes (NO LOS USAMOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0d5fb",
   "metadata": {},
   "source": [
    "### 2. Preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0259113",
   "metadata": {},
   "source": [
    "Reutilizamos código de la práctrica anterior. Al tener que presentarlas imágenes como un mapa bidimensional, lo mantenemos en su forma orginial. \n",
    "Como antes teníamos las imágenes aplanadas, usábamos np.array() para pasarlo a numpy. Ahora como tenemos las dimansiones originales, tenemos que usar tf.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bae5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de train_inputs: (5000, 96, 96, 3)\n",
      "Shape de train_targets: (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocesado(imagen, etiqueta):\n",
    "    imagen = tf.cast(imagen, tf.float32) / 255.0  # imagen a float32 y escala [0,1]\n",
    "    etiqueta = tf.one_hot(etiqueta, depth = num_clases) # one-hot encoding\n",
    "    return imagen, etiqueta\n",
    "\n",
    "def preprocesado_dataset(dataset):\n",
    "    # Creamos listas vacías donde almacenaremos las imágenes y etiquetas preprocesadas.\n",
    "    imagenes = []\n",
    "    etiquetas = []\n",
    "\n",
    "    for img, label in dataset: \n",
    "        imagen, etiqueta = preprocesado(img, label) # Aplicamos el preprocesado a cada muestra \n",
    "        # Añadimos los resultafos a las listas \n",
    "        imagenes.append(imagen)\n",
    "        etiquetas.append(etiqueta)\n",
    "\n",
    "    # Convertimos a arrays de numpy (manteniendo las 3 dimensiones)\n",
    "    return tf.stack(imagenes).numpy(), tf.stack(etiquetas).numpy() \n",
    "\n",
    "# Aplicamos la funcion a los conjuntos de entrenamiento y test\n",
    "train_inputs, train_targets = preprocesado_dataset(train)\n",
    "test_inputs, test_targets = preprocesado_dataset(test)\n",
    "\n",
    "\n",
    "print(\"Shape de train_inputs:\", train_inputs.shape)\n",
    "print(\"Shape de train_targets:\", train_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b8cfd",
   "metadata": {},
   "source": [
    "####  División del dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461ca1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutación aleatoria de índices para mezclar los datos\n",
    "# Para que la división train/validation sea aleatoria\n",
    "indices_permutation = np.random.permutation(len(train_inputs))\n",
    "shuffled_inputs = train_inputs[indices_permutation]\n",
    "shuffled_targets = train_targets[indices_permutation]\n",
    "\n",
    "# Calculamos cuántas muestras coger para validación (20% del de entrenamiento)\n",
    "num_validation_samples = int(0.2 * len(train_inputs))\n",
    "\n",
    "# Separamos el primer 20% para validación\n",
    "val_inputs = shuffled_inputs[:num_validation_samples]\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "# El resto se mantiene para entrenamiento\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca331c22",
   "metadata": {},
   "source": [
    "### 3. Creación y entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b3ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(modelo, train, val, test, epochs=15):\n",
    "    \n",
    "    # Desempaquetamos los conjuntos de datos\n",
    "    train_x, train_y = train\n",
    "    val_x, val_y = val\n",
    "    test_x, test_y = test\n",
    "    \n",
    "    # Entrenamos el modelo\n",
    "    history = modelo.fit(\n",
    "        train_x, train_y,\n",
    "        validation_data=(val_x, val_y),\n",
    "        epochs=epochs,\n",
    "        batch_size=128\n",
    "    )\n",
    "    \n",
    "    # Evaluamos el modelo en test y mostramos resultados\n",
    "    loss, acc, prec, rec, f1 = modelo.evaluate(test_x, test_y, verbose=0)\n",
    "    \n",
    "    print(\"\\nResultados en TEST:\")\n",
    "    print(f\"Loss: {loss:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    \n",
    "    return history, loss, acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7eea600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    accuracy = history.history[\"accuracy\"]\n",
    "    val_accuracy = history.history[\"val_accuracy\"]\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "    # Crear figura con 2 subplots lado a lado\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gráfica de accuracy\n",
    "    ax1.plot(epochs, accuracy, \"b-o\", label=\"Training accuracy\")\n",
    "    ax1.plot(epochs, val_accuracy, \"r--o\", label=\"Validation accuracy\")\n",
    "    ax1.set_title(\"Training and validation accuracy\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfica de loss\n",
    "    ax2.plot(epochs, loss, \"b-o\", label=\"Training loss\")\n",
    "    ax2.plot(epochs, val_loss, \"r--o\", label=\"Validation loss\")\n",
    "    ax2.set_title(\"Training and validation loss\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335f72c",
   "metadata": {},
   "source": [
    "#### 3.1. Modelo 1 - CNN base \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
