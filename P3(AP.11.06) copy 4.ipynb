{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4538d3bd",
   "metadata": {},
   "source": [
    "# Práctica 3 - Redes Neuronales Residuales "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be869758",
   "metadata": {},
   "source": [
    "# CARRERA DE MIERDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34098200",
   "metadata": {},
   "source": [
    "### Natalia Martínez García, Lucía Vega Navarrete\n",
    "### Grupo: AP.11.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5f3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, metrics, optimizers, losses\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c825b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para poder reproducir los resultados\n",
    "seed=1234\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc412d",
   "metadata": {},
   "source": [
    "### 1. Carga y preprocesado del dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1100a3",
   "metadata": {},
   "source": [
    "#### Carga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b624ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño total del dataset (ejemplos, columnas): (11000000, 29)\n",
      "División del dataset:\n",
      "Train: (2000000, 28)\n",
      "Test:  (500000, 28)\n",
      "Extra: (8500000, 28)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"HIGGS.csv.gz\") # El archivo está en la misma carpeta que el script\n",
    "\n",
    "# Cargar el dataset \n",
    "higgs = pd.read_csv(DATA_PATH, compression=\"gzip\", header=None)\n",
    "\n",
    "print(\"Tamaño total del dataset (ejemplos, columnas):\", higgs.shape)\n",
    "\n",
    "# Separar características (X) y etiquetas (y)\n",
    "#   - Columna 0: label\n",
    "#   - Columnas 1-28: features\n",
    "X = higgs.iloc[:, 1:].to_numpy(dtype=np.float32) \n",
    "y = higgs.iloc[:, 0].to_numpy(dtype=np.float32)\n",
    "\n",
    "# División según especificaciones: train inicial, test final, extra intermedio\n",
    "N_TRAIN = 2000000\n",
    "N_TEST = 500000\n",
    "\n",
    "X_train = X[:N_TRAIN]\n",
    "y_train = y[:N_TRAIN]\n",
    "\n",
    "X_test  = X[-N_TEST:]\n",
    "y_test  = y[-N_TEST:]\n",
    "\n",
    "X_extra = X[N_TRAIN:-N_TEST]\n",
    "y_extra = y[N_TRAIN:-N_TEST]\n",
    "\n",
    "print(\"División del dataset:\")\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Test:  {X_test.shape}\")\n",
    "print(f\"Extra: {X_extra.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e16b2",
   "metadata": {},
   "source": [
    "#### Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37761442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Normalización completada\n",
      "- Media global train (ya normalizado): -0.00000\n",
      "- Std global train  (ya normalizado): 1.00000\n",
      "\n",
      " Distribución de clases:\n",
      "\n",
      "Train:\n",
      "Signal = 1058818 (52.94%)\n",
      "Background = 941182   (47.06%)\n",
      "\n",
      "Extra:\n",
      "Signal = 4505798 (53.01%)\n",
      "Background = 3994202   (46.99%)\n",
      "\n",
      "Test:\n",
      "Signal = 264507 (52.90%)\n",
      "Background = 235493   (47.10%)\n"
     ]
    }
   ],
   "source": [
    "# Normalizar usando solo TRAIN\n",
    "# Media y desviación por columna (feature) \n",
    "#    (x - media_train) / std_train\n",
    "\n",
    "\"\"\"\n",
    "Normalizamos solo en train porque cualquier información del test no puede influir en el entrenamiento (el modelo solo puede ver estadísticas del train).\n",
    "Si usamos la media y desviación del test para normalizar, meteríamos información del futuro dentro del entrenamiento.\n",
    "\n",
    "Eso se llama data leakage (filtración de información) y hace que tus resultados no sean realistas.\n",
    "\"\"\"\n",
    "mean_train = X_train.mean(axis=0) # axis = 0 para calcularlo por columnas (cada feature)\n",
    "std_train  = X_train.std(axis=0)\n",
    "\n",
    "# Evitar división por cero\n",
    "std_train[std_train == 0] = 1.0\n",
    "\n",
    "# Aplicar la transformación estándar: (x - media) / sd\n",
    "X_train = (X_train - mean_train) / std_train\n",
    "X_extra = (X_extra - mean_train) / std_train\n",
    "X_test  = (X_test  - mean_train) / std_train\n",
    "\n",
    "print(\"\\n Normalización completada\")\n",
    "print(f\"- Media global train (ya normalizado): {X_train.mean():.5f}\")\n",
    "print(f\"- Std global train  (ya normalizado): {X_train.std():.5f}\")\n",
    "\n",
    "# Comprobación del desbalanceo de clases\n",
    "def class_stats(name, y):\n",
    "    n = len(y)\n",
    "    n_signal = np.sum(y == 1)\n",
    "    n_back   = np.sum(y == 0)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Signal = {n_signal} ({100*n_signal/n:.2f}%)\")\n",
    "    print(f\"Background = {n_back}   ({100*n_back/n:.2f}%)\")\n",
    "\n",
    "print(\"\\n Distribución de clases:\")\n",
    "class_stats(\"Train\", y_train)\n",
    "class_stats(\"Extra\", y_extra)\n",
    "class_stats(\"Test\",  y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c6e4b",
   "metadata": {},
   "source": [
    "### 2. Creación de red neuronal con capas residuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f79e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Bloque residual con:\n",
    "      - 2 capas Dense internas\n",
    "      - 1 conexión residual (skip connection)\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Crea las capas cuando se conoce input_shape\"\"\"\n",
    "        self.dense1 = layers.Dense(self.units, activation=self.activation, name=f\"{self.name}_dense1\")\n",
    "        self.dense2 = layers.Dense(self.units, activation=self.activation, name=f\"{self.name}_dense2\")\n",
    "        self.add = layers.Add(name=f\"{self.name}_add\")\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x_skip = inputs\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.add([x, x_skip])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"units\": self.units, \"activation\": self.activation})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bc1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiggsResNet(keras.Model):\n",
    "    \"\"\"\n",
    "    Red neuronal residual para clasificación del dataset Higgs.\n",
    "    Arquitectura:\n",
    "      - Capa de entrada (proyección a dimensión oculta)\n",
    "      - 2 bloques residuales\n",
    "      - Capa intermedia\n",
    "      - Capa de salida (sigmoid para clasificación binaria)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=28, hidden_dim=128, intermediate_dim=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        \n",
    "        # Definir todas las capas\n",
    "        self.entrada = layers.Dense(hidden_dim, activation=\"relu\", name=\"capa_entrada\")\n",
    "        self.res_block1 = ResidualBlock(hidden_dim, name=\"res_block1\")\n",
    "        self.res_block2 = ResidualBlock(hidden_dim, name=\"res_block2\")\n",
    "        self.intermedia = layers.Dense(intermediate_dim, activation=\"relu\", name=\"capa_intermedia\")\n",
    "        self.salida = layers.Dense(1, activation=\"sigmoid\", name=\"capa_salida\")\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Forward pass del modelo\"\"\"\n",
    "        x = self.entrada(inputs)\n",
    "        x = self.res_block1(x, training=training)\n",
    "        x = self.res_block2(x, training=training)\n",
    "        x = self.intermedia(x)\n",
    "        outputs = self.salida(x)\n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_dim\": self.hidden_dim,\n",
    "            \"intermediate_dim\": self.intermediate_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324fedeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de entrada: 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"higgs_resnet\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"higgs_resnet\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ capa_entrada (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ res_block1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ res_block2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa_intermedia (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa_salida (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ capa_entrada (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │         \u001b[38;5;34m3,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ res_block1 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ ?                      │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ res_block2 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ ?                      │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa_intermedia (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa_salida (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,081</span> (305.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,081\u001b[0m (305.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,081</span> (305.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,081\u001b[0m (305.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]  \n",
    "print(\"Tamaño de entrada:\", input_dim)\n",
    "\n",
    "# Crear el modelo usando la clase\n",
    "model = HiggsResNet(input_dim=input_dim, hidden_dim=128, intermediate_dim=64, name=\"higgs_resnet\")\n",
    "\n",
    "# Construir el modelo llamándolo con datos de ejemplo\n",
    "_ = model(X_train[:1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2f6fb",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cb46a",
   "metadata": {},
   "source": [
    "En esta práctica hemos implementado el entrenamiento de la red siguiendo la estructura del tutorial “Escribir un ciclo de entrenamiento desde cero” de TensorFlow, proporcionado en el enunciado de esta práctica.\n",
    "El código hace exactamente los mismos bloques lógicos propuestos en el tutorial, lo único diferente es que o ampliamos un poco para el cálculo de las métricas F1Score y balanced accurcay. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501000f",
   "metadata": {},
   "source": [
    "1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd699c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 28), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 28), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4096\n",
    "\n",
    "# Dataset de entrenamiento: barajado + dividido en batches\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100_000, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "\n",
    "# Dataset de test: solo en batches, sin shuffle\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89095e",
   "metadata": {},
   "source": [
    "2. Definir loss, optimizer y métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9bc597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=1e-3) # Optimizador\n",
    "loss_fn = losses.BinaryCrossentropy() # Función de pérdida (binaria, porque es 0/1)\n",
    "\n",
    "\n",
    "# Métricas para TRAIN\n",
    "train_loss = metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = metrics.BinaryAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "# Métricas para TEST\n",
    "test_loss = metrics.Mean(name=\"test_loss\")\n",
    "test_accuracy = metrics.BinaryAccuracy(name=\"test_accuracy\")\n",
    "# Toca calcular el F1 a mano tb xq me estaba dando un error de dimensionalidad por usar la clase q lo hace directo de keras lmao \n",
    "test_precision = metrics.Precision(name=\"test_precision\")\n",
    "test_recall    = metrics.Recall(name=\"test_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5945b14",
   "metadata": {},
   "source": [
    "3. Esto está copiado tal cual el tutorial, se supone q hace el entrenamiento más rápido \n",
    "\n",
    "`@tf.function` convierte las funciones en grafos de TensorFlow optimizados, lo que hace que el entrenamiento sea más rápido y eficiente.\n",
    "\n",
    "`train_step(x, y)` realiza un paso completo de entrenamiento (forward, cálculo de loss, gradientes, actualización de pesos y de métricas).\n",
    "\n",
    "`test_step(x, y)` realiza la evaluación sobre un batch sin actualizar los pesos, solo actualizando las métricas de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33decfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    \"\"\"Paso de entrenamiento: forward + loss + backprop + métricas train.\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss_value = loss_fn(y, predictions)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # Actualizar métricas de entrenamiento\n",
    "    train_loss.update_state(loss_value)\n",
    "    train_accuracy.update_state(y, predictions)\n",
    "    return loss_value\n",
    "    \n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    \"\"\"\n",
    "    Paso de evaluación:\n",
    "      - forward\n",
    "      - actualizar métricas de test (loss, accuracy, precision, recall)\n",
    "    Devuelve las predicciones para poder calcular balanced accuracy fuera.\n",
    "    \"\"\"\n",
    "    predictions = model(x, training=False)\n",
    "    loss_value = loss_fn(y, predictions)\n",
    "\n",
    "    test_loss.update_state(loss_value)\n",
    "    test_accuracy.update_state(y, predictions)\n",
    "    test_precision.update_state(y, predictions)\n",
    "    test_recall.update_state(y, predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ae8d1",
   "metadata": {},
   "source": [
    "4. Bucle de epochs + F1 score + balanced accuracy + guardado "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b352f",
   "metadata": {},
   "source": [
    "La estructura es la misma que el tutorial:\n",
    "* Loop de validación:\n",
    "    * for x_batch_val, y_batch_val in test_ds: val_predictions = test_step(...)\n",
    "* Leer métricas .result().\n",
    "* Resetear métricas (reset_state()).\n",
    "* Imprimir métricas + tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa1a08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "START OF EPOCH 1\n",
      "\n",
      "Training loss (for one batch) at step 0: 0.8607\n",
      "Seen so far: 4096 samples\n",
      "Training loss (for one batch) at step 200: 0.5656\n",
      "Seen so far: 823296 samples\n",
      "Training loss (for one batch) at step 400: 0.5359\n",
      "Seen so far: 1642496 samples\n",
      "\n",
      "--- Training metrics ---\n",
      "Accuracy: 0.6940\n",
      "Loss: 0.5745\n",
      "\n",
      "--- Validation metrics ---\n",
      "Loss: 0.5409\n",
      "Accuracy: 0.7241\n",
      "Precision: 0.7163\n",
      "Recall: 0.7923\n",
      "F1Score: 0.7524\n",
      "\n",
      "--- Balanced Accuracy ---\n",
      "Class 0 (background): 0.6476\n",
      "Class 1 (signal): 0.7923\n",
      "Media: 0.7199\n",
      "\n",
      "Time taken: 8.28s\n",
      "--------------------------------------------------\n",
      "\n",
      "START OF EPOCH 2\n",
      "\n",
      "Training loss (for one batch) at step 0: 0.5365\n",
      "Seen so far: 4096 samples\n",
      "Training loss (for one batch) at step 200: 0.5389\n",
      "Seen so far: 823296 samples\n",
      "Training loss (for one batch) at step 400: 0.5326\n",
      "Seen so far: 1642496 samples\n",
      "\n",
      "--- Training metrics ---\n",
      "Accuracy: 0.7331\n",
      "Loss: 0.5269\n",
      "\n",
      "--- Validation metrics ---\n",
      "Loss: 0.5169\n",
      "Accuracy: 0.7390\n",
      "Precision: 0.7478\n",
      "Recall: 0.7643\n",
      "F1Score: 0.7560\n",
      "\n",
      "--- Balanced Accuracy ---\n",
      "Class 0 (background): 0.7105\n",
      "Class 1 (signal): 0.7643\n",
      "Media: 0.7374\n",
      "\n",
      "Time taken: 7.66s\n",
      "--------------------------------------------------\n",
      "\n",
      "START OF EPOCH 3\n",
      "\n",
      "Training loss (for one batch) at step 0: 0.5030\n",
      "Seen so far: 4096 samples\n",
      "Training loss (for one batch) at step 200: 0.5146\n",
      "Seen so far: 823296 samples\n",
      "Training loss (for one batch) at step 400: 0.5035\n",
      "Seen so far: 1642496 samples\n",
      "\n",
      "--- Training metrics ---\n",
      "Accuracy: 0.7434\n",
      "Loss: 0.5110\n",
      "\n",
      "--- Validation metrics ---\n",
      "Loss: 0.5057\n",
      "Accuracy: 0.7464\n",
      "Precision: 0.7515\n",
      "Recall: 0.7777\n",
      "F1Score: 0.7644\n",
      "\n",
      "--- Balanced Accuracy ---\n",
      "Class 0 (background): 0.7112\n",
      "Class 1 (signal): 0.7777\n",
      "Media: 0.7444\n",
      "\n",
      "Time taken: 7.57s\n",
      "--------------------------------------------------\n",
      "\n",
      "Modelo guardado en: higgs_resnet_trained.keras\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 3 \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"\\nSTART OF EPOCH %d\" % (epoch + 1,))\n",
    "    print(\"\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # FASE DE ENTRENAMIENTO\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # Log cada 200 batches \n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * BATCH_SIZE))\n",
    "\n",
    "    # Mostrar las métricas al final de cada epoch \n",
    "    train_acc  = float(train_accuracy.result())\n",
    "    train_loss_epoch = float(train_loss.result())\n",
    "    print(\"\\n--- Training metrics ---\")\n",
    "    print(\"Accuracy: %.4f\" % train_acc)\n",
    "    print(\"Loss: %.4f\" % train_loss_epoch)\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "\n",
    "    # FASE DE EVALUACIÓN (TEST)\n",
    "    # Esto lo usamos luego para el cálculo de balances accuracy \n",
    "    y_true_all = [] # todas las estiquetas reales \n",
    "    y_pred_all = [] # todas las predicciones binarias \n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        # test_step actualiza loss, accuracy, precision, recall\n",
    "        val_predictions = test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "        # Guardar para balanced accuracy\n",
    "        y_true_all.append(y_batch_val.numpy().astype(np.int32))\n",
    "        y_pred_all.append((val_predictions.numpy() >= 0.5).astype(np.int32))\n",
    "\n",
    "    y_true_all = np.concatenate(y_true_all, axis=0).reshape(-1)\n",
    "    y_pred_all = np.concatenate(y_pred_all, axis=0).reshape(-1)\n",
    "\n",
    "    # Métricas de test a partir de los objetos de Keras \n",
    "    val_loss = float(test_loss.result())\n",
    "    val_acc = float(test_accuracy.result())\n",
    "    precision = float(test_precision.result())\n",
    "    recall = float(test_recall.result())\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    # Reset de métricas de test para la siguiente época\n",
    "    test_loss.reset_state()\n",
    "    test_accuracy.reset_state()\n",
    "    test_precision.reset_state()\n",
    "    test_recall.reset_state()\n",
    "\n",
    "    # Balanced Accuracy (a mano)\n",
    "    # identificamos los índices donde las etiquetas son 0/1\n",
    "    mask_0 = (y_true_all == 0) # pone a True todos los valores de la lista de etiquetas reales que valgan 0\n",
    "    mask_1 = (y_true_all == 1) # pone a True todos los valores de la lista de etiquetas reales que valgan 1\n",
    "\n",
    "    acc_class0 = np.mean(y_pred_all[mask_0] == y_true_all[mask_0])\n",
    "    acc_class1 = np.mean(y_pred_all[mask_1] == y_true_all[mask_1])\n",
    "    balanced_accuracy = (acc_class0 + acc_class1) / 2\n",
    "\n",
    "    # Log de validación como en el tutorial, pero ampliado \n",
    "    print(\"\\n--- Validation metrics ---\")\n",
    "    print(\"Loss: %.4f\" % val_loss)\n",
    "    print(\"Accuracy: %.4f\" % val_acc)\n",
    "    print(\"Precision: %.4f\" % precision)\n",
    "    print(\"Recall: %.4f\" % recall)\n",
    "    print(\"F1Score: %.4f\" % f1_score)\n",
    "\n",
    "    print(\"\\n--- Balanced Accuracy ---\")\n",
    "    print(\"Class 0 (background): %.4f\" % acc_class0)\n",
    "    print(\"Class 1 (signal): %.4f\" % acc_class1)\n",
    "    print(\"Media: %.4f\" % balanced_accuracy)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "MODEL_PATH = \"higgs_resnet_trained.keras\"\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"\\nModelo guardado en: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f18beb",
   "metadata": {},
   "source": [
    "FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "model_base = keras.models.load_model(\n",
    "    \"higgs_resnet_trained.keras\",\n",
    "    custom_objects={'ResidualBlock': ResidualBlock, 'HiggsResNet': HiggsResNet}\n",
    ")\n",
    "\n",
    "# Congelar TODAS las capas\n",
    "for layer in model_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Verificar que están congeladas (NO SE SI DEJAR ESTO ??????)\n",
    "print(\"Capas congeladas:\")\n",
    "for layer in model_base.layers:\n",
    "    print(f\"  - {layer.name}: trainable = {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AÑADIR MÓDULOS DE ADAPTACIÓN DE BAJA DIMENSIÓN \n",
    "# CLASE PORQUE VAMOS A REPETIR ESTO EN CADA CAPA ????????????\n",
    "\n",
    "class LoRADense(layers.Layer):\n",
    "    \"\"\"\n",
    "    Capa Dense con adaptación LoRA (Low-Rank Adaptation).\n",
    "    \n",
    "    Funciona así:\n",
    "      y = (W + α*B*A) * x + b\n",
    "    \n",
    "    Donde:\n",
    "      - W son los pesos originales (congelados)\n",
    "      - A y B son matrices pequeñas (entrenables)\n",
    "      - α controla cuánta influencia tiene la adaptación\n",
    "    \"\"\"\n",
    "    def __init__(self, dense_layer, rank=4, alpha=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense_layer = dense_layer\n",
    "        self.rank = rank  # r en la fórmula\n",
    "        self.alpha = alpha  # α en la fórmula\n",
    "        \n",
    "        # Congelar la capa densa original ( ESTO ES REDUNDANTE ??????)\n",
    "        self.dense_layer.trainable = False\n",
    "        \n",
    "        # Dimensiones de los pesos originales W\n",
    "        self.units = dense_layer.units  # dout\n",
    "        self.input_dim = None  # din (se determina en build)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[-1]  # din\n",
    "        \n",
    "        # Crear matrices A y B de baja dimensión\n",
    "        # A: (r × din) - inicializada con valores aleatorios\n",
    "        self.lora_A = self.add_weight(\n",
    "            name='lora_A',\n",
    "            shape=(self.rank, self.input_dim),\n",
    "            # NOTA: MIRAR SI ESTO DE VERDAD ES INICIALIZACION ALEATORIA\n",
    "            initializer='glorot_uniform',  # Valores aleatorios\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # B: (dout × r) - inicializada a CERO\n",
    "        self.lora_B = self.add_weight(\n",
    "            name='lora_B',\n",
    "            shape=(self.units, self.rank),\n",
    "            initializer='zeros',  # Todo a cero\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Salida de la capa original (W*x + b)\n",
    "        base_output = self.dense_layer(inputs)\n",
    "        \n",
    "        # Calcular la adaptación: α * B * A * x\n",
    "        # Paso 1: A * x  → resultado tiene forma (batch, r)\n",
    "        # tf.matmul es una función de TensorFlow que multiplica dos matrices\n",
    "        lora_output = tf.matmul(inputs, self.lora_A, transpose_b=True)\n",
    "        \n",
    "        # Paso 2: B * (A * x)  → resultado tiene forma (batch, dout)\n",
    "        lora_output = tf.matmul(lora_output, self.lora_B, transpose_b=True)\n",
    "        \n",
    "        # Aplicar el factor α\n",
    "        lora_output = self.alpha * lora_output\n",
    "        \n",
    "        # Combinar: (W*x + b) + α*B*A*x\n",
    "        return base_output + lora_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'rank': self.rank,\n",
    "            'alpha': self.alpha\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43bb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTO CAMBIARLO FIJO. POR QUE HACER UNA CLASE NUEVA ??????'\n",
    "\n",
    "# Función para crear modelo con LoRA\n",
    "def create_lora_model(base_model, rank=4, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Envuelve todas las capas Dense del modelo base con LoRA.\n",
    "    \"\"\"\n",
    "    # Envolver cada capa Dense con LoRA\n",
    "    lora_entrada = LoRADense(base_model.entrada, rank=rank, alpha=alpha, name=\"lora_entrada\")\n",
    "    \n",
    "    lora_res1_d1 = LoRADense(base_model.res_block1.dense1, rank=rank, alpha=alpha, name=\"lora_res1_dense1\")\n",
    "    lora_res1_d2 = LoRADense(base_model.res_block1.dense2, rank=rank, alpha=alpha, name=\"lora_res1_dense2\")\n",
    "    \n",
    "    lora_res2_d1 = LoRADense(base_model.res_block2.dense1, rank=rank, alpha=alpha, name=\"lora_res2_dense1\")\n",
    "    lora_res2_d2 = LoRADense(base_model.res_block2.dense2, rank=rank, alpha=alpha, name=\"lora_res2_dense2\")\n",
    "    \n",
    "    lora_intermedia = LoRADense(base_model.intermedia, rank=rank, alpha=alpha, name=\"lora_intermedia\")\n",
    "    lora_salida = LoRADense(base_model.salida, rank=rank, alpha=alpha, name=\"lora_salida\")\n",
    "    \n",
    "    # Crear el modelo adaptado\n",
    "    class HiggsResNetLoRA(keras.Model):\n",
    "        def __init__(self, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.lora_entrada = lora_entrada\n",
    "            self.lora_res1_d1 = lora_res1_d1\n",
    "            self.lora_res1_d2 = lora_res1_d2\n",
    "            self.lora_res2_d1 = lora_res2_d1\n",
    "            self.lora_res2_d2 = lora_res2_d2\n",
    "            self.lora_intermedia = lora_intermedia\n",
    "            self.lora_salida = lora_salida\n",
    "            self.add_layer = layers.Add()\n",
    "        \n",
    "        def call(self, inputs, training=None):\n",
    "            # Capa de entrada\n",
    "            x = self.lora_entrada(inputs)\n",
    "            \n",
    "            # Bloque residual 1\n",
    "            x_skip = x\n",
    "            x = self.lora_res1_d1(x)\n",
    "            x = self.lora_res1_d2(x)\n",
    "            x = self.add_layer([x, x_skip])\n",
    "            \n",
    "            # Bloque residual 2\n",
    "            x_skip = x\n",
    "            x = self.lora_res2_d1(x)\n",
    "            x = self.lora_res2_d2(x)\n",
    "            x = self.add_layer([x, x_skip])\n",
    "            \n",
    "            # Capas finales\n",
    "            x = self.lora_intermedia(x)\n",
    "            outputs = self.lora_salida(x)\n",
    "            \n",
    "            return outputs\n",
    "    \n",
    "    return HiggsResNetLoRA()\n",
    "\n",
    "\n",
    "# Crear el modelo con LoRA\n",
    "RANK = 8  # Valor pequeño (prueba con 4, 8, 16)\n",
    "ALPHA = 0.1  # Factor de escala (0.1 funciona bien)\n",
    "\n",
    "model_lora = create_lora_model(model_base, rank=RANK, alpha=ALPHA)\n",
    "\n",
    "# Construir el modelo\n",
    "_ = model_lora(X_train[:1])\n",
    "\n",
    "# Contar parámetros\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model_lora.trainable_weights])\n",
    "total_params = sum([tf.size(w).numpy() for w in model_lora.weights])\n",
    "\n",
    "print(f\"\\nParámetros del modelo LoRA:\")\n",
    "print(f\"  - Total: {total_params:,}\")\n",
    "print(f\"  - Entrenables (solo LoRA): {trainable_params:,}\")\n",
    "print(f\"  - Congelados (modelo base): {total_params - trainable_params:,}\")\n",
    "print(f\"  - Porcentaje entrenable: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIRAR SI SE PUEDE ENTRENAR CON FIT O NO \n",
    "\n",
    "# ENTRENAR EL MODELO ADAPTADO CON LOS DATOS EXTRA\n",
    "print(\"\\n=== PASO 4c: Entrenando modelo LoRA ===\\n\")\n",
    "\n",
    "# Preparar datos extra\n",
    "extra_dataset = tf.data.Dataset.from_tensor_slices((X_extra, y_extra))\n",
    "extra_dataset = extra_dataset.shuffle(buffer_size=100_000).batch(BATCH_SIZE)\n",
    "\n",
    "# Compilar modelo LoRA\n",
    "model_lora.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        metrics.BinaryAccuracy(name='accuracy'),\n",
    "        metrics.Precision(name='precision'),\n",
    "        metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Entrenar (menos épocas porque solo ajustamos parámetros pequeños)\n",
    "EPOCHS_LORA = 2\n",
    "\n",
    "history_lora = model_lora.fit(\n",
    "    extra_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS_LORA,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModelo LoRA entrenado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2eb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4d. EVALUAR Y COMPARAR RESULTADOS\n",
    "print(\"\\n=== PASO 4d: Evaluación y comparación ===\\n\")\n",
    "\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    \"\"\"Evalúa un modelo y calcula todas las métricas\"\"\"\n",
    "    predictions = model.predict(X, verbose=0)\n",
    "    y_pred_binary = (predictions >= 0.5).astype(np.int32).reshape(-1)\n",
    "    y_true = y.astype(np.int32)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Precision y Recall\n",
    "    tp = np.sum((y_pred_binary == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred_binary == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred_binary == 0) & (y_true == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "    # Balanced Accuracy\n",
    "    mask_0 = (y_true == 0)\n",
    "    mask_1 = (y_true == 1)\n",
    "    acc_class0 = np.mean(y_pred_binary[mask_0] == y_true[mask_0])\n",
    "    acc_class1 = np.mean(y_pred_binary[mask_1] == y_true[mask_1])\n",
    "    balanced_acc = (acc_class0 + acc_class1) / 2\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy:          {accuracy:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"  Precision:         {precision:.4f}\")\n",
    "    print(f\"  Recall:            {recall:.4f}\")\n",
    "    print(f\"  F1-Score:          {f1_score:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "# Evaluar modelo base (sin adaptación)\n",
    "results_base = evaluate_model(model_base, X_test, y_test, \"MODELO BASE (sin fine-tuning)\")\n",
    "\n",
    "# Evaluar modelo con LoRA\n",
    "results_lora = evaluate_model(model_lora, X_test, y_test, \"MODELO CON LoRA (con fine-tuning)\")\n",
    "\n",
    "# Comparación\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACIÓN DE MEJORAS\")\n",
    "print(\"=\"*60)\n",
    "for metric in ['accuracy', 'balanced_accuracy', 'f1_score']:\n",
    "    improvement = results_lora[metric] - results_base[metric]\n",
    "    print(f\"{metric.replace('_', ' ').title():20s}: {improvement:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4e. COMPACTAR W, A y B EN UNA ÚNICA MATRIZ\n",
    "print(\"\\n=== PASO 4e: Compactando LoRA en pesos originales ===\\n\")\n",
    "\n",
    "def merge_lora_weights(model_lora):\n",
    "    \"\"\"\n",
    "    Combina W + α*B*A en una única matriz para cada capa.\n",
    "    Esto hace que el modelo sea igual de rápido que el original.\n",
    "    \"\"\"\n",
    "    print(\"Fusionando pesos LoRA con pesos base...\")\n",
    "    \n",
    "    # Lista de capas LoRA\n",
    "    lora_layers = [\n",
    "        model_lora.lora_entrada,\n",
    "        model_lora.lora_res1_d1,\n",
    "        model_lora.lora_res1_d2,\n",
    "        model_lora.lora_res2_d1,\n",
    "        model_lora.lora_res2_d2,\n",
    "        model_lora.lora_intermedia,\n",
    "        model_lora.lora_salida\n",
    "    ]\n",
    "    \n",
    "    for lora_layer in lora_layers:\n",
    "        # Obtener pesos originales W\n",
    "        original_weights = lora_layer.dense_layer.get_weights()\n",
    "        W = original_weights[0]  # Matriz de pesos\n",
    "        b = original_weights[1]  # Bias\n",
    "        \n",
    "        # Obtener matrices LoRA\n",
    "        A = lora_layer.lora_A.numpy()  # (r × din)\n",
    "        B = lora_layer.lora_B.numpy()  # (dout × r)\n",
    "        alpha = lora_layer.alpha\n",
    "        \n",
    "        # Calcular: W_new = W + α*B*A\n",
    "        # B*A resulta en una matriz (dout × din), igual que W\n",
    "        BA = np.matmul(B, A)  # (dout × r) @ (r × din) = (dout × din)\n",
    "        W_new = W + alpha * BA.T  # Transponer porque TensorFlow usa (din × dout)\n",
    "        \n",
    "        # Actualizar pesos de la capa original\n",
    "        lora_layer.dense_layer.set_weights([W_new, b])\n",
    "        \n",
    "        print(f\"  ✓ {lora_layer.name}: fusionado\")\n",
    "    \n",
    "    print(\"\\nTodos los pesos LoRA han sido fusionados con los pesos base\")\n",
    "    return model_lora\n",
    "\n",
    "# Fusionar pesos\n",
    "model_merged = merge_lora_weights(model_lora)\n",
    "\n",
    "# Verificar que da el mismo resultado\n",
    "print(\"\\nVerificando que el modelo fusionado da los mismos resultados...\")\n",
    "pred_lora = model_lora.predict(X_test[:1000], verbose=0)\n",
    "pred_merged = model_merged.predict(X_test[:1000], verbose=0)\n",
    "\n",
    "difference = np.abs(pred_lora - pred_merged).max()\n",
    "print(f\"Diferencia máxima entre predicciones: {difference:.10f}\")\n",
    "\n",
    "if difference < 1e-5:\n",
    "    print(\"✓ Las predicciones son idénticas (diferencia despreciable)\")\n",
    "else:\n",
    "    print(\"⚠ Hay diferencias significativas\")\n",
    "\n",
    "# Evaluar modelo fusionado\n",
    "results_merged = evaluate_model(model_merged, X_test, y_test, \"MODELO FUSIONADO (W + α*B*A)\")\n",
    "\n",
    "# Guardar modelo final\n",
    "model_merged.save(\"higgs_resnet_finetuned.keras\")\n",
    "print(\"\\nModelo fusionado guardado en: higgs_resnet_finetuned.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140310cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
