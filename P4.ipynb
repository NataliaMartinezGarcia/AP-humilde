{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 4 - Recurrent Neural Networks y Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natalia Martínez García, Lucía Vega Navarrete\n",
    "### Grupo: AP.11.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos todas las librerías que vamos a necesitar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciaveganavarrete/anaconda3/envs/AP/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from miditok import REMI, TokenizerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carga y preprocesado del dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos el ejemplo del enunciado de la práctica para crear el Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el tokenizador REMI\n",
    "config = TokenizerConfig()\n",
    "tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las rutas de los datos\n",
    "MIDI_DIR = \"midi\"\n",
    "TRAIN_DIR = os.path.join(MIDI_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(MIDI_DIR, \"test\")\n",
    "\n",
    "# Obtener lista de archivos midi\n",
    "train_files = sorted([os.path.join(TRAIN_DIR, f) for f in os.listdir(TRAIN_DIR) if f.endswith('.mid')])\n",
    "test_files = sorted([os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if f.endswith('.mid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(token_ids, tokenizer, seq_length=51):\n",
    "    \"\"\"\n",
    "    Crea secuencias de longitud fija a partir de los tokens de un archivo midi.\n",
    "    - Si hay menos de 51 tokens: rellena con EOS + PAD\n",
    "    - Si hay más de 51 tokens: usa ventanas deslizantes\n",
    "    \"\"\"\n",
    "    sequences = [] # Lista para guardar todas las secuencias\n",
    "    \n",
    "    # Obtener los ids de los tokens especiales\n",
    "    # Los vamos a usar cuando el midi no llegue a 51 tokens\n",
    "    eos_token = tokenizer['EOS_None'] # marca el final de una secuencia\n",
    "    pad_token = tokenizer['PAD_None'] # token de relleno\n",
    "    \n",
    "    # si la canción tiene menos de 51 tokens rellenar\n",
    "    if len(token_ids) < seq_length:\n",
    "        # Añadir el token EOS al final para marcar donde termina\n",
    "        sequence = token_ids + [eos_token]\n",
    "        # Calcular cuántos tokens PAD necesitamos para llegar a 51\n",
    "        padding_needed = seq_length - len(sequence)\n",
    "        # Añadir ese numero de pads\n",
    "        sequence = sequence + [pad_token] * padding_needed\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # Si tiene 51 o mas tokens usar ventanas deslizantes\n",
    "    else:\n",
    "        # extrae 51 tokens desde la primera posición\n",
    "        # pasa a la segunda y extrae 51 desde esa posición, etc, etc\n",
    "        for i in range(len(token_ids) - seq_length + 1):\n",
    "            sequence = token_ids[i:i + seq_length]\n",
    "            sequences.append(sequence)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función toma los tokens de un archivo MIDI y los organiza en ventanas de 51 tokens:\n",
    "- **Si el archivo tiene menos de 51 tokens**: lo rellenamos con tokens especiales (EOS + PAD)\n",
    "- **Si el archivo tiene más de 51 tokens**: creamos ventanas deslizantes. La ventana se ma moviendo token a token: primero cogemos los tokens del 1 al 51, luego movemos la ventana una posición y cogemos del 2 al 52, después del 3 al 53, y así sucesivamente hasta llegar al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(file_list, tokenizer, seq_length=51):\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos MIDI y crea las secuencias\n",
    "    \"\"\"\n",
    "    all_sequences = [] # Lista para guardar secuencias de todos los archivos\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        # Tokenizar el archivo\n",
    "        tokens = tokenizer(file_path)\n",
    "        # Obtener los ids del primer canal (índice 0)\n",
    "        token_ids = tokens[0].ids\n",
    "        # Crear secuencias de longitud fija con la funcion de arriba\n",
    "        sequences = create_sequences(token_ids, tokenizer, seq_length)\n",
    "        # Añadir todas las secuencias de este archivo a la lista total\n",
    "        all_sequences.extend(sequences)\n",
    "    # Convertir a numpy\n",
    "    return np.array(all_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función procesa todos los archivos MIDI de una carpeta (train o test). Primero lee cada archivo MIDI, lo tokeniza y extrae solo el primer canal. Luego crea las ventanas de 51 tokens para cada archivo y guarda todas las secuencias juntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(sequences):\n",
    "    \"\"\"\n",
    "    Para una ventana de 51 tokens:\n",
    "    - Entrada: los primeros 50 tokens\n",
    "    - Etiqueta: los últimos 50 tokens\n",
    "    \"\"\"\n",
    "    X = sequences[:, :50] # Primeros 50 tokens (del 0 al 49)\n",
    "    y = sequences[:, 1:] # Últimos 50 tokens (del token 1 al 50)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función separa cada ventana de 51 tokens en entrada y salida, ya que vamos a hacer redes que predigan el siguiente token:\n",
    "- **Entrada (X)**: los primeros 50 tokens (posiciones 0 a 49). Lo que la red recibe como contexto\n",
    "- **Etiqueta (y)**: los últimos 50 tokens (posiciones 1 a 50). Lo que la red debe predecir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procesamos de esta manera los conjuntos de entrenamiento y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencias de entrenamiento creadas: 2894912\n",
      "Forma de X_train: (2894912, 50)\n",
      "Forma de y_train: (2894912, 50)\n",
      "Secuencias de test creadas: 139315\n",
      "Forma de X_test: (139315, 50)\n",
      "Forma de y_test: (139315, 50)\n"
     ]
    }
   ],
   "source": [
    "# Procesar el conjunto de entrenamiento\n",
    "# tokenizar cada archivo y crea ventanas de 51 tokens\n",
    "train_sequences = process_dataset(train_files, tokenizer, seq_length=51)\n",
    "# Separar en entrada y salida con ventanas\n",
    "X_train, y_train = prepare_data_for_training(train_sequences)\n",
    "\n",
    "print(f\"Secuencias de entrenamiento creadas: {len(train_sequences)}\")\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "\n",
    "# Procesar el conjunto de test\n",
    "# tokenizar cada archivo y crea ventanas de 51 tokens\n",
    "test_sequences = process_dataset(test_files, tokenizer, seq_length=51)\n",
    "# Separar en entrada y salida con ventanas\n",
    "X_test, y_test = prepare_data_for_training(test_sequences)\n",
    "\n",
    "print(f\"Secuencias de test creadas: {len(test_sequences)}\")\n",
    "print(f\"Forma de X_test: {X_test.shape}\")\n",
    "print(f\"Forma de y_test: {y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
